import torch
import logging
from diffusers.utils import load_image
from diffusers.models.embeddings import apply_rotary_emb
import numpy as np
import cv2
import argparse
import os
from pipelines.pipelines_imageedit import FluxKontextPipeline
from diffusers.utils import load_image
from types import MethodType
from diffusers.models.embeddings import apply_rotary_emb
import math
import matplotlib.cm as cm
import numpy as np
from PIL import Image
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
from skimage.metrics import structural_similarity as ssim
from diffusers.training_utils import compute_density_for_timestep_sampling

# export HF_HOME=/data/CVPR_Public/huggingface

def create_folder_with_full_permissions(path: str):
    os.makedirs(path, exist_ok=True)
    os.chmod(path, 0o777)

def image_comparison(image1: np.ndarray, image2: np.ndarray) -> float:
    """
    Compare two images using SSIM (Structural Similarity Index).
    Both images must be grayscale and have the same shape.
    """
    # Resize image2 to match image1
    image2 = cv2.resize(image2, (image1.shape[1], image1.shape[0]), interpolation=cv2.INTER_AREA)

    # SSIM expects float images in [0, 255] or normalized [0, 1]
    score = ssim(image1, image2, data_range=255)
    return score

def image_comparison_ssim(image1: np.ndarray, image2: np.ndarray) -> float:
    """ image_similarity_sift
    ÌÅ¥ ÏàòÎ°ù Ïú†ÏÇ¨Ìïú Í∞íÏù¥Îã§. Í∞íÏùò Î≤îÏúÑ 0 ~ 1
    Compare two images using SIFT feature matching.
    Returns a similarity score: (number of good matches) / (min number of keypoints).
    """

    # Ïù¥ÎØ∏ÏßÄÎ•º Í∑∏Î†àÏù¥Ïä§ÏºÄÏùºÎ°ú Î≥ÄÌôò
    gray1 = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY) if len(image1.shape) == 3 else image1
    gray2 = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY) if len(image2.shape) == 3 else image2

    # SIFT ÎîîÏä§ÌÅ¨Î¶ΩÌÑ∞ ÏÉùÏÑ±
    sift = cv2.SIFT_create()

    # ÌäπÏßïÏ†ê Î∞è ÎîîÏä§ÌÅ¨Î¶ΩÌÑ∞ Ï∂îÏ∂ú
    kp1, des1 = sift.detectAndCompute(gray1, None)
    kp2, des2 = sift.detectAndCompute(gray2, None)

    if des1 is None or des2 is None:
        return 0.0  # ÌäπÏßïÏ†êÏù¥ ÏóÜÎäî Í≤ΩÏö∞

    # BFMatcher ÏÉùÏÑ± Î∞è Îß§Ïπ≠
    bf = cv2.BFMatcher(cv2.NORM_L2)
    matches = bf.knnMatch(des1, des2, k=2)

    # Lowe's ratio test Ï†ÅÏö©
    good_matches = [m for m, n in matches if m.distance < 0.75 * n.distance]

    # Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞: Ï¢ãÏùÄ Îß§Ïπ≠ Ïàò / Í∞ÄÎä•Ìïú ÏµúÏÜå ÎîîÏä§ÌÅ¨Î¶ΩÌÑ∞ Ïàò
    min_des = min(len(des1), len(des2))
    if min_des == 0:
        return 0.0

    similarity = len(good_matches) / min_des
    return similarity

SIB = [18, 19, 31, 41]
QIB = [1, 17, 27, 32, 56]
OIB = [i for i in range(19 * 3) if i not in SIB and i not in QIB]

ETB = [1, 18]
OTB = [i for i in range(19 * 3) if i not in ETB]

class Controller():

    def __init__(self):

        self.attention_dict = {}
        self.layer_timenum_dict = {}
        self.skiptime = None
        self.original_forward = ""
        self.img_alpha_dict = {}
        self.txt_alpha_dict = {}
        self.state = ''
        self.text_len = 0
        self.infer_time_dict = {}

    def add_infer_time(self, block_idx):
        if block_idx not in self.infer_time_dict:
            self.infer_time_dict[block_idx] = 1
        self.infer_time_dict[block_idx] = self.infer_time_dict[block_idx] + 1

    def set_state(self, state):
        self.state = state

    def set_alpha(self, attn_idx, txt_alpha, img_alpha):
        self.txt_alpha_dict[attn_idx] = txt_alpha
        self.img_alpha_dict[attn_idx] = img_alpha

    def reset(self):
        self.attention_dict = {}
        self.layer_timenum_dict = {}
        self.skiptime = None
        self.original_forward = ""
        self.img_alpha_dict = {}
        self.txt_alpha_dict = {}
        self.state = ''
        self.text_len = 0
        self.infer_time_dict = {}

    def save_attn_map(self, timestep, attn_map):
        if timestep not in self.attention_dict:
            self.attention_dict[timestep] = []
        self.attention_dict[timestep].append(attn_map)

def main(args):

    print(f' step 1. Call VLM')
    device = 'cuda'
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct",
                                                               torch_dtype="auto",
                                                               device_map=device)
    processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-7B-Instruct", ) #cache_dir='/data2/CVPR_Public/model/Qwen')

    print(f' step 2. Call Flux Pipe')
    pipe = FluxKontextPipeline.from_pretrained("black-forest-labs/FLUX.1-Kontext-dev",
                                               torch_dtype=torch.bfloat16,)
    pipe.to('cuda')
    flux_transformer = pipe.transformer

    print(f' (2.1) register new function')
    def onetime_inference(self, latents, image_embeds, image_latents, t, i, guidance,
                          pooled_prompt_embeds,
                          prompt_embeds, text_ids, latent_ids, callback_on_step_end,
                          callback_on_step_end_tensor_inputs, timesteps, num_warmup_steps, progress_bar,
                          height, width, output_type, start_noise,
                          logger,
                          structure_ideal_low, structure_ideal_high,change_num ):
        second_infer = 0
        self._current_timestep = t
        latent_model_input = latents
        latent_model_input = torch.cat([latents, image_latents], dim=1)
        timestep = t.expand(latents.shape[0]).to(latents.dtype)
        save_time = int(timestep.round().item())
        noise_pred = self.transformer(
            hidden_states=latent_model_input,
            timestep=timestep / 1000,
            guidance=guidance,
            pooled_projections=pooled_prompt_embeds,
            encoder_hidden_states=prompt_embeds,
            txt_ids=text_ids,
            img_ids=latent_ids,
            joint_attention_kwargs=self.joint_attention_kwargs,
            return_dict=False, )[0]
        noise_pred = noise_pred[:, : latents.size(1)]

        # -------------------------------------------------------------------------------------
        # [1] EDIF-S Module
        # -------------------------------------------------------------------------------------
        def predict_x0(start_noise, noise_pred, present_latent, do_save=True, do_newname=False):
            x0_pred_latent = start_noise - noise_pred
            x0_pred_im = self._unpack_latents(x0_pred_latent, 1024, 1024, self.vae_scale_factor)
            x0_pred_im = (x0_pred_im / self.vae.config.scaling_factor) + self.vae.config.shift_factor
            x0_pred_im = self.vae.decode(x0_pred_im, return_dict=False)[0]
            x0_pred_pil = self.image_processor.postprocess(x0_pred_im, output_type='pil')[0]
            if do_save:
                if do_newname:
                    x0_pred_pil.save(os.path.join(inter_folder, f'x0_pred_{save_time}_new.png'))
                else:
                    x0_pred_pil.save(os.path.join(inter_folder, f'x0_pred_{save_time}.png'))
            # noise_pred Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•
            noise_pred_im = self._unpack_latents(noise_pred, 1024, 1024, self.vae_scale_factor)
            noise_pred_im = (noise_pred_im / self.vae.config.scaling_factor) + self.vae.config.shift_factor
            noise_pred_im = self.vae.decode(noise_pred_im, return_dict=False)[0]
            noise_pred_pil = self.image_processor.postprocess(noise_pred_im, output_type='pil')[0]

            present_latent_im = self._unpack_latents(present_latent, 1024, 1024, self.vae_scale_factor)
            present_latent_im = (present_latent_im / self.vae.config.scaling_factor) + self.vae.config.shift_factor
            present_latent_im = self.vae.decode(present_latent_im, return_dict=False)[0]
            present_latent_pil = self.image_processor.postprocess(present_latent_im, output_type='pil')[0]
            if do_save:
                if do_newname:
                    present_latent_pil.save(os.path.join(inter_folder, f'present_latent_{save_time}_new.png'))
                else:
                    present_latent_pil.save(os.path.join(inter_folder, f'present_latent_{save_time}.png'))
            return x0_pred_latent, x0_pred_pil, present_latent_pil
        def set_compare_tensor(inference_step, x0_pred, x0_pred_pil, present_latent, present_latent_pil ):
            if inference_step > -1:
                if save_time > 300:
                    compare_tensor = x0_pred
                    compare_pil = x0_pred_pil
                else:
                    compare_tensor = present_latent
                    compare_pil = present_latent_pil
            return compare_tensor, compare_pil
        def get_sigmas(i, n_dim=4, dtype=torch.float32):
            sigmas = np.linspace(1.0, 1 / num_inference_steps, num_inference_steps)
            sigmas = torch.tensor(sigmas, device=image_latents.device, dtype=dtype)
            sigma = sigmas[i].flatten()
            while len(sigma.shape) < n_dim:
                sigma = sigma.unsqueeze(-1)
            return sigma
        x0_pred_latent, x0_pred_pil, present_latent_pil = predict_x0(start_noise, noise_pred, latents, do_save=True)
        compare_tensor, compare_pil = set_compare_tensor(timestep, x0_pred_latent, x0_pred_pil, latents, present_latent_pil)
        compare_pil.save(os.path.join(inter_folder, f'comparison_{save_time}.png'))

        def get_structural_similarity(compare_pil, input_image):
            image1 = np.array(compare_pil.resize((1024, 1024)).convert('L'))
            image2 = np.array(input_image.resize((1024, 1024)).convert('L'))
            structure_similarity = image_comparison(image1, image2)
            return structure_similarity
        structure_similarity = get_structural_similarity(compare_pil, input_image)

        # -------------------------------------------------------------------------------------
        # [2] EDIF-E Module
        def EDIF_E_Module_Decision(max_retry):
            def check_text_editing():
                edited_img_dir = os.path.join(inter_folder, f'comparison_{save_time}.png')
                conversation = [{"role": "user",
                                 "content": [
                                     {"type": "image", "path": edited_img_dir},
                                     {"type": "text", "text": f"Ïù¥ Ïù¥ÎØ∏ÏßÄÏóê {user_edit_keyword} Í∞Ä Ïûò ÌëúÌòÑÎêòÏóàÎãà?"
                                                              f"Ïûò ÌëúÌòÑÎêòÏóàÎã§Î©¥ Yes Î°ú ÎãµÌïòÍ≥† ÏïÑÎãàÎ©¥ No Î°ú ÎãµÌï¥Ï§ò. Îã§Î•∏ ÎãµÏùÄ ÎßêÍ≥†, Yes ÏôÄ No Î°úÎßå ÎãµÌï¥Ï§å"}, ], }]
                inputs = processor.apply_chat_template(conversation, video_fps=1, add_generation_prompt=True,
                                                       tokenize=True, return_dict=True, return_tensors="pt").to(model.device)
                output_ids = model.generate(**inputs, max_new_tokens=128)
                generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]
                output_text = processor.batch_decode(generated_ids, skip_special_tokens=True,
                                                     clean_up_tokenization_spaces=True)[0]
                return output_text.strip().lower()
            txt_decision = None
            for attempt in range(max_retry):
                txt_decision = check_text_editing()
                if txt_decision.lower() in ['yes', 'no']:
                    break
                else:
                    logger.info(f"[{attempt + 1}/{max_retry}] Unexpected response: '{txt_decision}' ‚Üí retrying...")
            return txt_decision
        # -------------------------------------------------------------------------------------
        logger.info(f"[{save_time}] structure_similarity: {structure_similarity}")
        ideal_structure_similar_range = (structure_ideal_low, structure_ideal_high)

        delta_img = 0.05
        if structure_similarity > ideal_structure_similar_range[1]:  # too similar
            second_infer += 1
            logger.info(f'[‚ö†] structure_similarity {structure_similarity} is over ideal ({ideal_structure_similar_range[1]}) ‚Üí image ‚Üì')
            for k in controller.img_alpha_dict:
                old_img_alpha = controller.img_alpha_dict[k]
                if k in QIB:
                    new_img_alpha = old_img_alpha
                    action = "Ïú†ÏßÄ (QIB)"
                elif k in SIB:
                    new_img_alpha = max(0.6, old_img_alpha - delta_img)
                    action = f"‚Üì {delta_img:.2f} (SIB)"
                else:
                    new_img_alpha = max(0.6, old_img_alpha - delta_img * 0.5)
                    action = f"‚Üì {delta_img * 0.5:.2f} (Other)"
                controller.img_alpha_dict[k] = new_img_alpha
                if k == 0:
                    logger.info(f'  [‚Üì] img_alpha[{k}]: {old_img_alpha:.2f} ‚Üí {new_img_alpha:.2f} | {action}')
        elif structure_similarity < ideal_structure_similar_range[0]:
            second_infer += 1
            logger.info(f'[‚ö†] structure_similarity {structure_similarity} is too low ideal ({ideal_structure_similar_range[0]}) ‚Üí image ‚Üë')
            for k in controller.img_alpha_dict:
                old_img_alpha = controller.img_alpha_dict[k]
                if k in QIB:
                    new_img_alpha = old_img_alpha
                    action = "Ïú†ÏßÄ (QIB)"
                elif k in SIB:
                    new_img_alpha = min(2.0, old_img_alpha + delta_img)
                    action = f"‚Üë {delta_img:.2f} (SIB)"
                else:
                    new_img_alpha = min(2.0, old_img_alpha + delta_img * 0.5)
                    action = f"‚Üë {delta_img * 0.5:.2f} (Other)"
                controller.img_alpha_dict[k] = new_img_alpha
                if k == 0:
                    logger.info(f'  [‚Üë] img_alpha[{k}]: {old_img_alpha:.2f} ‚Üí {new_img_alpha:.2f} | {action}')
        else :
            for k in controller.img_alpha_dict:
                old_img_alpha = controller.img_alpha_dict[k]
                controller.img_alpha_dict[k] = 1
            action = "Ïú†ÏßÄ (QIB)"
            logger.info(f'  [good] img_alpha[{k}]: {old_img_alpha:.2f} ‚Üí 1.00 | {action}')
        # -------------------------------------------------------------------------------------
        # [2] EDIF-E Module
        # -------------------------------------------------------------------------------------
        max_retry = 3
        txt_decision = EDIF_E_Module_Decision(max_retry)
        logger.info(f'VLM txt_decision = {txt_decision}')
        if txt_decision.lower() not in ['yes', 'no']:
            logger.info(f"[‚úò] Failed to get valid decision after {max_retry} attempts: '{txt_decision}'")
        delta_txt = 0.1  # Ï°∞Ï†ï Î≤îÏúÑ
        # SIB = [18, 19, 31, 41]
        # QIB = [1, 17, 27, 32, 56]
        # OIB = [i for i in range(19 * 3) if i not in SIB and i not in QIB]
        # ETB = [1, 18]
        # OTB = [i for i in range(19 * 3) if i not in ETB]
        if txt_decision.strip().lower() == 'no':
            second_infer += 1
            logger.info(f"[‚úò] {user_edit_keyword} Í∞Ä Ïûò ÌëúÌòÑÎêòÏßÄ ÏïäÏùå ‚Üí ÌÖçÏä§Ìä∏ ÏïåÌåå ‚Üë")
            for k in controller.txt_alpha_dict:
                old_txt_alpha = controller.txt_alpha_dict[k]
                if k in ETB:
                    new_txt_alpha = min(2.0, old_txt_alpha + delta_txt * 1.0)
                    action = f"‚Üë {delta_txt:.2f} (ETB)"
                else:
                    new_txt_alpha = min(2.0, old_txt_alpha + delta_txt * 0.5)
                    action = f"‚Üë {delta_txt * 0.5:.2f} (Other)"
                controller.txt_alpha_dict[k] = new_txt_alpha
                if k == 0:
                    logger.info(f"  [‚Üë] txt_alpha[{k}]: {old_txt_alpha:.2f} ‚Üí {new_txt_alpha:.2f} | {action}")
        elif txt_decision.strip().lower() == 'yes':
            #if structure_similarity > structure_ideal_low and change_num == 0 and i > 0 : # use good
            #    print(f'change original latents !!!!!!!!!!!!!!!!!')
            #    image_latents = compare_tensor
            #    change_num += 1
            # for k in controller.txt_alpha_dict:
            #    old_txt_alpha = controller.txt_alpha_dict[k]
            #    new_txt_alpha = 1
            #    controller.txt_alpha_dict[k] = 1
            logger.info(f"[‚úì] '{user_edit_keyword}' Í∞Ä Ïûò ÌëúÌòÑÎê® ‚Üí txt Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ + start latent change")
        else:
            logger.info(f"[!] ÌÖçÏä§Ìä∏ Ïù∏Ïãù Ïã§Ìå® ÎòêÎäî Ïù¥ÏÉÅ ÏùëÎãµ: {txt_decision.strip()} ‚Üí check_text_editing() Ïû¨ÏãúÎèÑ ÎòêÎäî ÏòàÏô∏ Ï≤òÎ¶¨ ÌïÑÏöî")

        img_alpha_dict = controller.img_alpha_dict
        txt_alpha_dict = controller.txt_alpha_dict
        first_img_key = sorted(img_alpha_dict.keys())[0]
        first_txt_key = sorted(txt_alpha_dict.keys())[0]
        logger.info(f'# Ï≤´Î≤àÏß∏ img alpha dict: key = {first_img_key}, value = {img_alpha_dict[first_img_key]}')
        logger.info(f'# Ï≤´Î≤àÏß∏ txt alpha dict: key = {first_txt_key}, value = {txt_alpha_dict[first_txt_key]}')
        latents_dtype = latents.dtype
        latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]
        if latents.dtype != latents_dtype:
            if torch.backends.mps.is_available():
                latents = latents.to(latents_dtype)
        if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):
            if progress_bar is not None:
                progress_bar.update()
        logger.info(f' ============================================================================ ')
        return latents, image_latents, change_num

    pipe.onetime_inference = MethodType(onetime_inference, pipe)

    print(f' step 2. Edit with Total')
    def ca_forward(attn_module, layer_idx, controller):
        def forward(hidden_states: torch.FloatTensor,
                    encoder_hidden_states: torch.FloatTensor = None,
                    attention_mask=None,
                    image_rotary_emb=None,
                    **kwargs) -> torch.FloatTensor:

            attn = attn_module
            is_cross = encoder_hidden_states is not None

            # alpha Ï°∞Ï†ï
            txt_alpha = controller.txt_alpha_dict.get(layer_idx, 1.0)
            img_alpha = controller.img_alpha_dict.get(layer_idx, 1.0)

            if is_cross:
                encoder_hidden_states *= txt_alpha
                total_len = hidden_states.size(1)
                source_len = total_len // 2
                image_hidden_states = hidden_states[:, -source_len:] * img_alpha
                hidden_states[:, -source_len:] = image_hidden_states
            else:
                hidden_states[:, :512] *= txt_alpha
                total_len = hidden_states.size(1)
                source_len = (total_len - 512) // 2
                hidden_states[:, -source_len:] *= img_alpha

            batch_size, _, _ = hidden_states.shape if not is_cross else encoder_hidden_states.shape
            query = attn.to_q(hidden_states)
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)

            # head Î∂ÑÌï†
            inner_dim = key.shape[-1]
            head_dim = inner_dim // attn.heads
            query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
            value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)

            if attn.norm_q: query = attn.norm_q(query)
            if attn.norm_k: key = attn.norm_k(key)

            # encoder projection (cross-attn Ïãú)
            if is_cross:
                def proj(tensor, proj_fn, norm_fn):
                    out = proj_fn(tensor).view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
                    return norm_fn(out) if norm_fn else out

                query = torch.cat([proj(encoder_hidden_states, attn.add_q_proj, attn.norm_added_q), query], dim=2)
                key = torch.cat([proj(encoder_hidden_states, attn.add_k_proj, attn.norm_added_k), key], dim=2)
                value = torch.cat([proj(encoder_hidden_states, attn.add_v_proj, None), value], dim=2)

            if image_rotary_emb is not None:
                query = apply_rotary_emb(query, image_rotary_emb)
                key = apply_rotary_emb(key, image_rotary_emb)

            def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0,
                                             is_causal=False, use_selective_attention=True, bos_index=0):
                B, H, L, D = query.shape
                scale = 1.0 / math.sqrt(D)
                attn_logits = torch.matmul(query, key.transpose(-2, -1)) * scale
                attn_weights = torch.softmax(attn_logits, dim=-1)

                # -------------------- üîç Visualize & Adjust --------------------
                def visualize_attention_maps(attn_weights, text_len, own_len, source_len, save_path):
                    B, H, L, _ = attn_weights.shape
                    pixel_start = text_len
                    pixel_own_end = pixel_start + own_len
                    pixel_source_start = pixel_own_end

                    # 1. Text attention
                    attn_text = attn_weights[:, :, pixel_start:pixel_own_end, :text_len]
                    attn_text_sum = attn_text.sum(dim=-1, keepdim=True)

                    # 2. Self attention
                    attn_self = attn_weights[:, :, pixel_start:pixel_own_end, pixel_start:pixel_own_end]
                    diag_mask = torch.eye(own_len, device=attn_self.device).unsqueeze(0).unsqueeze(0)
                    attn_self_diag = (attn_self * diag_mask).sum(dim=-1, keepdim=True)

                    # 3. Source attention
                    attn_source = attn_weights[:, :, pixel_start:pixel_own_end, pixel_source_start:]
                    diag_mask2 = torch.eye(source_len, device=attn_source.device).unsqueeze(0).unsqueeze(0)
                    attn_source_diag = (attn_source * diag_mask2).sum(dim=-1, keepdim=True)

                    # Mean & Squeeze
                    attn_text_v = attn_text_sum.mean(dim=(0, 1)).squeeze()
                    attn_self_v = attn_self_diag.mean(dim=(0, 1)).squeeze()
                    attn_source_v = attn_source_diag.mean(dim=(0, 1)).squeeze()

                    stacked = torch.stack([attn_text_v, attn_self_v, attn_source_v], dim=0)
                    stacked_norm = (stacked - stacked.min()) / (stacked.max() - stacked.min() + 1e-8)

                    grid_size = int(own_len ** 0.5)
                    stacked_2d = stacked_norm.reshape(3, grid_size, grid_size)

                    # Colormap + Resize
                    colormap = cm.get_cmap("jet")
                    img_list = []
                    for i in range(3):
                        heat = (stacked_2d[i].cpu().numpy() * 255).astype(np.uint8)
                        heat_colored = colormap(heat / 255.0)[..., :3]
                        img_colored = Image.fromarray((heat_colored * 255).astype(np.uint8)).resize((512, 512))
                        img_list.append(img_colored)

                    # Combine
                    composite = Image.new('RGB', (512 * 3, 512))
                    for i, img in enumerate(img_list):
                        composite.paste(img, (i * 512, 0))
                    #composite.save(save_path)

                # save & count
                layer_timenum = controller.layer_timenum_dict.get(layer_idx, 0)
                save_path = os.path.join(save_dir,f'attn_idx_{layer_idx}_time_{layer_timenum}.png')
                visualize_attention_maps(attn_weights, text_len=512, own_len=4096, source_len=4096,
                                         save_path=save_path)
                controller.layer_timenum_dict[layer_idx] = layer_timenum + 1
                # ------------------------------------------------------------

                attn_weights = torch.dropout(attn_weights, dropout_p, train=True)
                return torch.matmul(attn_weights, value)

            # attention Ï†ÅÏö© ÌõÑ ÌõÑÏ≤òÎ¶¨
            hidden_states = scaled_dot_product_attention(query, key, value, attn_mask=attention_mask)
            hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
            hidden_states = hidden_states.to(query.dtype)

            if is_cross :
                encoder_hidden_states, hidden_states = hidden_states.split_with_sizes(
                    [encoder_hidden_states.shape[1], hidden_states.shape[1] - encoder_hidden_states.shape[1]], dim=1
                )
                hidden_states = attn.to_out[0](hidden_states)
                hidden_states = attn.to_out[1](hidden_states)
                encoder_hidden_states = attn.to_add_out(encoder_hidden_states)

                return hidden_states, encoder_hidden_states
            else:
                return hidden_states

        return forward


    # ------------------------------------------------------------------------------------------------------------
    # Inference Base Code
    # ------------------------------------------------------------------------------------------------------------
    print(f' step 3. Inference Base Code')
    controller = Controller()
    def onestep_inference(input_image,
                          edit_prompt,
                          num_inference_steps,
                          save_dir,save_name,
                          structure_ideal_low,
                          structure_ideal_high, ):
        # [1] set controller
        attention_idx = 0
        for name, module in flux_transformer.named_modules():
            if module.__class__.__name__ == 'FluxAttention' or module.__class__.__name__ == 'Attention':
                txt_alpha = 1
                img_alpha = 1
                controller.set_alpha(attention_idx, txt_alpha, img_alpha)
                module.forward = ca_forward(module, attention_idx, controller)
                attention_idx += 1
        def setup_logger(log_path):
            os.makedirs(os.path.dirname(log_path), exist_ok=True)
            logger = logging.getLogger(os.path.basename(log_path))  # Ïù¥Î¶Ñ Ï§ëÎ≥µ Î∞©ÏßÄ
            logger.setLevel(logging.INFO)
            file_handler = logging.FileHandler(log_path, mode='a')  # ÎòêÎäî 'w'
            formatter = logging.Formatter('%(asctime)s - %(message)s')
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter('[%(levelname)s] %(message)s')
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)
            return logger
        # [2] set logger
        log_dir = os.path.join(save_dir, f'record_log.log')
        logger = setup_logger(log_dir)
        # [3] inference setting
        generator = torch.Generator().manual_seed(args.seed)
        img = pipe(image=input_image,
                   prompt=edit_prompt,
                   num_inference_steps=num_inference_steps,
                   generator=generator,
                   original=args.original,
                   save_dir=save_dir,
                   structure_ideal_low=structure_ideal_low,
                   structure_ideal_high=structure_ideal_high,
                   logger=logger,
                   guidance_scale=2.5).images[0]
        img.save(os.path.join(save_dir, save_name))

    input_image = Image.open(args.img_dir).resize((1024,1024))
    edit_prompt = args.edit_prompt
    save_dir = args.save_folder
    create_folder_with_full_permissions(save_dir)
    input_image.save(os.path.join(save_dir, f'original_real_image.png'))
    inter_folder = os.path.join(save_dir, 'inter')
    create_folder_with_full_permissions(inter_folder)
    user_edit_keyword = args.edit_prompt.split(' ')[-2]
    print(f'user_edit_keyword: {user_edit_keyword}')
    save_prompt = edit_prompt.replace(' ','_')
    image_name, _ = os.path.splitext(save_prompt)
    save_name = f'{image_name}_{save_prompt}.png'
    onestep_inference(input_image,
                      edit_prompt,
                      args.num_inference_steps,
                      save_dir,
                      save_name,
                      args.structure_ideal_low,
                      args.structure_ideal_high)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--pretrained_model', type=str, help='Path to the pretrained model directory')
    parser.add_argument('--save_folder', type=str, help='Directory to save results')
    parser.add_argument('--img_dir', type=str, help='Directory of input image')
    parser.add_argument('--edit_prompt', type=str, help='editing prompt')
    parser.add_argument('--similarity_score_threds', type=float, help='Similarity score threshold')
    parser.add_argument('--structure_ideal_low', type=float, default=0.3,
                                   help='Lower bound of ideal similarity range')
    parser.add_argument('--structure_ideal_high', type=float, default=0.9, help='Upper bound of ideal similarity range')
    parser.add_argument('--use_ssim', action='store_true', help='Use base model mode')
    parser.add_argument('--latent_change_threds',
                        type=int,default = 5)
    parser.add_argument('--num_inference_steps',type=int, default=28)
    parser.add_argument('--seed',type=int, default=1983)
    parser.add_argument('--original',action = 'store_true')
    args = parser.parse_args()
    main(args)